{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 1:中国的大学排名（定向爬虫)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import bs4\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHTMLText(url):\n",
    "    try:\n",
    "        r=requests.get(url,timeout=30)\n",
    "        r.raise_for_status\n",
    "        r.encoding=r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        return ''\n",
    "def fillUnivList(html):\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    result=pd.DataFrame(columns=('排名','大学名称','总分'))\n",
    "    for tr in soup.find('tbody').children:\n",
    "        if isinstance(tr, bs4.element.Tag):\n",
    "            tds=tr('td')\n",
    "            result=result.append(pd.DataFrame(np.matrix([tds[0].string,tds[1].string,tds[3].string]),columns=result.columns))\n",
    "    return result\n",
    "def main():\n",
    "    url='http://www.zuihaodaxue.cn/zuihaodaxuepaiming2016.html'\n",
    "    html=getHTMLText(url)\n",
    "    result=fillUnivList(html)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     排名     大学名称    总分\n",
      "0     1     清华大学  95.9\n",
      "0     2     北京大学  82.6\n",
      "0     3     浙江大学    80\n",
      "0     4   上海交通大学  78.7\n",
      "0     5     复旦大学  70.9\n",
      "..  ...      ...   ...\n",
      "0   305   新疆医科大学  27.2\n",
      "0   307  江西中医药大学  26.4\n",
      "0   308  景德镇陶瓷学院  26.1\n",
      "0   309     潍坊学院  23.6\n",
      "0   310     德州学院  21.5\n",
      "\n",
      "[310 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case2: 从百度股票获取股票数据,代码应该没问题，但是百度股票好像已经没了，然后东方财富那里换成js了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import traceback\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHTMLText(url):\n",
    "    try:\n",
    "        r=requests.get(url,timeout=30)\n",
    "        r.raise_for_status\n",
    "        r.encoding=r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        return ''\n",
    "def getStockList(lst,stockURL):\n",
    "    html=getHTMLText(stockURL)\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    a=soup.findAll('a')\n",
    "    for i in a:\n",
    "        try:\n",
    "            href=i.attrs['href']\n",
    "            lst.append(re.findall(r'[s][hz]\\d{6}',href)[0])\n",
    "        except:\n",
    "            continue\n",
    "def getStockInfo(lst,stockURL,fpath):\n",
    "    for stock in lst:\n",
    "        url=stockURL+stock+'.html'\n",
    "        html=getHTMLText(url)\n",
    "        try:\n",
    "            if html=='':\n",
    "                continue\n",
    "            infoDict={}\n",
    "            soup=BeautifulSoup(html,'html.parser')\n",
    "            stockInfo=soup.find('div', attrs={'class':'stock_bets'})\n",
    "            name=stockInfo.find_all(attrs={'class':'bets-name'})[0]\n",
    "            infoDict.update({'股票名称':name.text.split()[0]})\n",
    "            keyList=stockInfo.find_all('dt')\n",
    "            valueList=stockInfo.find_all('dd')\n",
    "            for i in range(len(keyList)):\n",
    "                key=keyList[i].text\n",
    "                val=valueList[i].text\n",
    "                infoDict[key]=val\n",
    "            with open(fpath,'a',encoding='utf-8') as f:\n",
    "                f.write(str(infoDict)+'\\n')\n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "def main():\n",
    "    stock_list_url='http://quote.eastmoney.com/stocklist.html'\n",
    "    stock_info_url='https://gupiao.baidu.com/stock/'\n",
    "    output_file='D:/Study/results/BaiduStockInfo.txt'\n",
    "    slist=[]\n",
    "    getStockList(slist,stock_list_url)\n",
    "    getStockInfo(slist,stock_info_url,output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SZ000001', 'SZ603555']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-29-2694ed1783f7>\", line 29, in getStockInfo\n",
      "    name=stockInfo.find_all(attrs={'class':'bets-name'})[0]\n",
      "AttributeError: 'NoneType' object has no attribute 'find_all'\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-29-2694ed1783f7>\", line 29, in getStockInfo\n",
      "    name=stockInfo.find_all(attrs={'class':'bets-name'})[0]\n",
      "AttributeError: 'NoneType' object has no attribute 'find_all'\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
