# Bagging 与 随机森林

Bagging 是一种集成学习的代表方法。

对于集成学习来说，我们希望每个基学习器是尽可能独立的，虽然在现实中我们没办法实现独立，但是我们可以使得学习器之间尽可能有差距（而不是完全一样，不然就失去了集成学习的效果了）。

一个可能的做法是对样本进行采样，产生若干个不同的子集，由于每个子集包含的观测是不一样的，训练出来的基学习器是可望具有较大的差异的。

Bagging 方法通过Bootstrap 方法进行重采样。假设样本有 n 个观测，我们每次通过有放回的采样抽取 n 个样本来进行训练，就得到了一个基学习器。总共抽取 T次，我们就得到了 T 个基学习器，结合这 T 个基学习器的结果，就得到最终的学习结果。对于分类问题，Bagging 使用简单投票法，对于回归问题，Bagging 使用 T个基学习器的简单平均。

Bagging 的重点在于获得一个方差比较小的集成模型。Bagging的思想其实对于任何的基学习器都可以使用，但是使用更多的还是对于决策树。Bagging 方法的一个好处是在于其很容易利用并行计算（每个基学习器的训练是独立的），因此可以很好的利用多核计算的优势。

> 在我看来，树模型是一种非常非常好的机器学习方法。单独的决策树的效果一般很差（很容易过拟合），但是一旦运用上 Bagging 或者 Boosting 的集成手段之后，树模型就得到了翻天覆地的变化。在很多的任务上表现出了非常好的性能。





随机森林是 Bagging 的一个变体。随机森林在 Bagging 的基础上，在决策树的训练过程中引入了随机的特征选择。假设总共有 p 个特征，在决策树的每次节点划分的时候，传统的决策树模型是在这 p 个特征中选择一个最优特征来进行分支，而随机森林中，我们先随机抽取 m 个特征， 然后在这 m 个特征中选择一个最优的特征来进行分支。



特征重要性:

随机森林的特征重要性一般有两种方法:

* 每个特征在划分过程中，会给Gini系数（或者信息增益）带来改变，累积所有树的所有的特征的划分作为该特征的重要性（然后进行一下标准化，保证所有特征重要性的求和是1.），这也是一般的Boosting 的树模型的变量重要性的计算方式。
* 对于随机森林模型，还有一种独特的用于衡量特征重要性的方法。我们可以使用袋外样本（out of bag sample) 来对特征重要性进行评级。我们可以利用袋外样本来计算袋外的准确性（也叫做袋外得分）。然后我们随机置换袋外样本中第 j 个特征的值，再次计算袋外得分，袋外得分的变化值作为第 j 个特征的重要性（单颗树），然后对所有树取平均，得到整个随机森林模型的第 j 个特征的重要性。



随机森林的调参

> P.S. 随机森林的调参的关键在于对于决策树的参数的选择，森林的参数不是很敏感。对决策树的调参，我们也可以发现，很多时候在默认的参数下就能取得不错的效果，随机森林对参数的敏感度相对没有其他的模型高（如NN）。

* 决策树的参数

* 随机森林的参数：

  * 树的个数：原则上来说，越多越好，但是得考虑计算，一般选个moderate的值就可以了

  * 每次随机选择的特征子集的数目，默认选择在分类问题中 $$m=\sqrt{p}$$，在回归问题中 $$m=p^{1/3}$$, 

    > 如果特征数目很少，可以增大子集的比例，如果特征数目太多，可以减少。在大部分的问题中，默认的设置就表现的不错

    

    



​    



