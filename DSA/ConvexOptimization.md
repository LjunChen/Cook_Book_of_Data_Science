# 凸优化理论

### 无约束凸优化

在无约束凸优化问题中，我们想要考虑的一个问题是
$$
\min_{x} f(x)
$$

这里，我们一般假设 $$f$$ 是凸的，并且是可导的（不可导的情况下可以参加Proximal Gradient部分）。

#### Gradient Descent method

梯度下降算法应该是凸优化问题中最常见的算法。梯度下降的基本思想很简单，而且网上到处都有，我就不加以介绍了。我主要介绍一下梯度下降算法的数学背景. 我们将 $$f(x)$$ 在 $$x_k$$这一点进行Taylor 展开
$$
f(x)\approx f(x_k)+\nabla f(x_k)(x-x_k)
$$
如果我们取 $$x=x_k+t_k \nabla f(x_k)$$, 则只要保证 $$t_k>0$$，则 $$f(x)<f(x_k)$$. 当然这种近似依赖于 $$x$$ 与 $$x_k$$ 非常接近，所以 $$t_k$$ 不能取太大。

> 梯度下降算法
>
> * 给一个初始点 $$x_0$$.
>
> 重复：
>
> * $$\Delta x_k=-\nabla f(x_k)$$
> * 选择 合适的步长 $$t_k$$
> * 迭代 $$x_{k+1}=x_k+t_k\Delta x_k$$.
>
> 直到达到收敛的条件。

对于这样的一个梯度下降算法，有2个问题需要考虑： 收敛的条件，以及步长的选择。

##### 收敛的条件

一般来说，收敛的条件一般有以下三种（不排除有其他收敛条件）:

* $$||x_{k+1}-x_{k}||<\varepsilon$$, 即当 $$x$$的变化很小的时候, 这里可以是1范数，也可以是2范数，都有人用
* $$|f(x_{k+1})-f(x_{k})|<\varepsilon$$, 即当目标函数变化很小的时候
* $$||\nabla f(x_k)||<\varepsilon$$, 即当梯度接近0的时候

##### 步长的选择

步长的选择是梯度下降算法的难点，也有很多学者在研究这类问题。步长过长，可能会导致算法没法收敛。步长太短，会导致收敛很慢。对于某些特定问题来说，可能有最优的步长，比如说，对于 $$\beta$$ smooth的目标函数，我们就可以将步长设置为 $$1/\beta$$. 但是实际上，更多的情况下，确定步长是一件非常困难的事情。一般来说，我们可以考虑用 backtracking 的方法选择步长。

> Backtracking 步长搜索策略
>
> 给定一个初始的 $$\beta\in (0,1)$$ 和 $$t_k$$,
>
> While $$f(x_k+t_k^{(0)}\Delta x)> f(x)$$,               $$t_k=\beta t_k$$.

这样选择的目的在于保证目标函数的值是在下降的。在目标函数不是特别复杂，数据量不是特别大的时候，其实这种步长的选择策略是很合适的。但是，当目标函数及其复杂并且数据量很大的时候，这种步长选择的策略会带来大量的计算成本。

这一点在深度学习领域特别明显（目标函数是非凸的，高度复杂，并且样本量一般极大).  因此，我们可能需要更好的选择步长的方式，目前的主流的步长选择方式应该是Adam? 在介绍Adam 之前，我们需要先介绍牛顿算法。

Adam=Adagrad + Momentum.

#### 牛顿算法

我们依旧考虑对 $$f(x)$$ 进行Taylor 展开，不过这次我们保留二阶的余项
$$
f(x)\approx f(x_k)+\nabla f(x_k)(x-x_k)+\dfrac{1}{2}(x-x_k)^T \nabla^2 f(x_k)(x-x_k)
$$
这对于 $$\Delta x=x-x_k$$ 是个二次优化的问题（我们想找$$\Delta x$$使得 $$f(x)$$最小）。我们可以令其一阶导数为0，即
$$
\dfrac{d}{d\Delta x}(f(x_k)+\nabla f(x_k)\Delta x+\dfrac{1}{2}\Delta x^T \nabla^2 f(x_k)\Delta x)=0,
$$
可以得到 
$$
\Delta x=-(\nabla^2 f(x_k))^{-1}\nabla f(x_k).
$$
一般来说，牛顿算法的收敛速度很快（主要在于用到了二阶导数的信息），但是牛顿算法涉及到二阶导数矩阵（Hessian 矩阵) 的计算， 这方面的计算消耗的成本很大，所以如果目标函数复杂的话，牛顿算法的收敛速度反而很慢。成也萧何败萧何！

因此有很多学者考虑对牛顿算法做了一些改进，如BFGS， 其主要思想都是不直接计算Hessian矩阵，而是对Hessian矩阵进行近似。

#### Adagrad

之所以介绍牛顿算法，是因为我觉得 Adagrad 用到了牛顿法的思想（所以也可以看成一种拟牛顿算法，不过好像大家不这么说。）

Adagrad 的学习率（也就是我们前面提到的步长 $$t_k$$, 叫法不一样，但是其实是一个东西）是自适应的（adaptive）.
$$
t_k=\dfrac{\eta}{\sqrt{\sum_{i=0}^{k}(\nabla f(x_i))^2}}
$$
Adagrad的参数更新是对每一个参数单独进行更新的，所以这里 $$x_i$$看成一维的就好了，如果不是一维的，就固定其他参数，一次更新一个。 Adagrad 可以看成是对牛顿方法的一种近似。分母部分是过去梯度的累积平方和.  Adagrad 的学习率是不断减小的。

#### Momentum

这个主要是考虑了惯性。

下降的方向=-a\*梯度方向+b\*上一步的方向

#### Adam

Adam =Adagrad +Momentum

目前在深度学习上最流行的梯度下降测量，由于深度学习的特点，一般不是用普通的梯度下降，而是用随机梯度下降（SGD），所以Adam 可以认为是 Adagrad, Momentum 和 SGD的 结合。 SGD暂时没时间写，有时间写一下。

