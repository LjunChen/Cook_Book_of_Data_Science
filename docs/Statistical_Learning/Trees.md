## 决策树

决策树常用的算法有 ID3 , C4.5 和 CART。

### 决策树的特征选择问题

##### 信息增益

我们一般用熵（这应该是来自于物理学的一个概念，熵表示一种混合程度）来度量随机变量的不确定性。设$$X$$ 是一个取有限值的离散随机变量，其概率分布为 $$P(X=x_i)=p_i, i=1,2,\ldots,n$$, 则$$X$$的熵为
$$
H(X)=-\sum_{i=1}^n p_i\log p_i
$$
熵越大则随机变量的不确定越大。我们接下来定义条件熵 $$H(Y|X)$$ 为
$$
H(Y|X)=\sum_{i=1}^n p_i H(Y|X_i),
$$
这里 $$p_i=P(X=x_i)$$.



选择我们考虑一个实际的背景： 假设我们有个数据集 D, 

*  用 |D| 表示样本容量， 样本有K个类 C1,C2,... ,Ck，

*  假设特征 X 有 m 个 不同的取值， 根据X的取值，可以将数据集 D划分为 m 个子集， D1，D2, ... Dm, 

  

> 信息增益：我们定义特征X给数据集带来的信息增益为
> $$
> g(D,X)=H(D)-H(D|X)
> $$
> 表示给定$$X$$给$$Y$$的熵带来的变化。

我们要进行变量选择，其实就是找出给$$Y$$带来的信息增益比较大的那些 $$X$$. 但是当我们用信息增益来进行变量选择时，会倾向于选择取值较多的特征，因此我们考虑对取值的可能性做一个惩罚，我们定义

> 信息增益比:
> $$
> g_R(D,X)=\dfrac{g(D,X)}{H_{X}(D)}
> $$
> 这里的 $$1/H_X(D)$$就相当于一个惩罚系数，他的定义为 $$H_X(D)=-\sum_{i=1}^m \dfrac{|D_i|}{|D|} \log(\dfrac{|D_i|}{|D|})$$.
>
> 可以理解为按照 X的取值对 D 进行划分得到的熵，如果 X的取值的可能性越多，则 这个值就越大。

##### Gini系数

> 我们定义随机变量$$X$$ Gini 系数为
> $$
> Gini(X)=Gini(p)=\sum_{k=1}^K p_k(1-p_k)=1-\sum_{k=1}^K p_k^2
> $$
> 对于给定的样本 $D$, 其基尼系数为
> $$
> Gini(D)=1-\sum_{k=1}^K (\dfrac{|C_k|}{D})^2.
> $$



### 决策树的生成算法

##### ID3决策树

ID3的核心是利用信息增益来进行特征的选择。从根节点开始，对该节点计算所有特征的信息增益，选择信息增益最大的特征作为此节点的特征，然后根据此特征的不同取值构造决策树的分支（这个特征有多少种取值，就会产生多少个分支）， 产生子节点，再对子节点用以上方法递归构建决策树，直到所有特征的信息增益都很小（可以设定一个阈值）或所有特征都选择完毕。此时得到一颗决策树。

最后根据每一片叶子（最终节点可以看成是叶子）上不同类别的数据观测个数（我们假设有K种可能的类别），选择数据观测个数最多的类作为这片叶子的标记。



##### C4.5决策树

C4.5 决策树和 ID3决策树的不同之处在于 C4.5使用信息增益比作为特征选择的准则。



### CART决策树

CART 决策树是基于Gini系数来构建的二叉决策树。对于每一个特征 A，对其所有可能的取值 a, 根据 A 是否等于 a 可以将样本分为两类，计算 A=a 时基尼系数。在所有可能的特征A中这么计算，找出最优特征及其对应的划分，作为此次的分支生成子节点。



> Remarks: 对于连续数据，我们一般都是将其离散化（最简单的就是用二分法，选定一个阈值，大于阈值的一类，小于阈值的一类）。对于缺失值，决策树原则上来说是可以处理缺失值的（也并不复杂），但是目前主流的关于决策树的包（比如sklearn）并没有实现这一功能，因此我们在实际处理的时候，还需要对缺失值进行填充，或者删除掉缺失的观测。



决策树算法是很容易过拟合的，因此决策树算法需要剪枝操作（包括预剪枝和后剪枝），但是同样，在sklearn里面也没有这些操作，我们一般是通过一些参数来控制决策树的深度等等。并且，一般在使用决策树算法时候，我们一般都是将决策树作为一个基算法，然后利用boosting/bagging来集成多棵决策树。



