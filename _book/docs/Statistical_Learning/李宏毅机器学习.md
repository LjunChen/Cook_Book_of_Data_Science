## Regression

* 加 Regularization的会使得参数的估计值比较小，会让估计出来的函数更加平滑（smooth)， 会比较少受noise的影响，对noise不会太sensitive, 因此加regularization一般不对bias进行，因为bias和平滑程度其实是没有关系的
* 相对来说，简单的模型方差较小，bias较大（underfitting），而复杂的模型方差较大，bias较小(overfitting)。如果training data的fit都不好是underfitting, 而training data表现好，但是在testing data表现不好的话是overfitting

##  Gradient Descent

##### Adagrad

* Divide the learning rate of each parameter by the root mean square of its previous derivates.
  $$
  w^{t+1}\leftarrow w^t-\dfrac{\eta^t}{\sigma^t}g^t
  $$

$$
\sigma_t=\sqrt{\frac{1}{t+1}\sum_{i=0}^{t}(g^{i})^2}, \eta^t=\dfrac{\eta}{\sqrt{t+1}}
$$

​	Then Adagrad can written as 
$$
w^{t+1}\leftarrow w^t-\dfrac{\eta}{\sqrt{\sum_{i=0}^{t}(g^{i})^2}}g^t
$$
下面的那个部分类似于对二次微分的近似。（考虑二次微分是更加合理的，类似Newton算法，但是直接计算二次微分一般比较困难）

##### Stochastic Gradient Descent

一次对一个参数进行update

##### Feature Scaling

一般来说，会比较有效率

## Generative Model V.S. Discriminative

Benefits of generative model:

* With the assumption of probabilty distribution, less training data is needed.
* With the assumption of probabilty distribution, more robust to noise.
* Priors and class-dependent probabilities can be estimated from different sources.



## Deep Learning vs Other Machine Learning

Machine Learning: Feature Transform, Feature Engineering

Deep Learning: Design the Structure 

所以使用传统的Machine Learning和 Deep Learning处决于哪个更难。

如果 Feature Engineering 很难，比如语音辨识，则Deep Learning可能更好。

但是如果 Feature Engineering 简单，则传统方法可能更加简单，会更好。



## Tips for Deep Learning

* Deep Learning其实不是特别容易overfitting, 很多时候对training data的拟合效果都很差，所以第一步要检查在training data的表现。 Dont't always blame overfitting.

* 有时候复杂的模型陷入了local mimima ，从而导致了复杂的模型看上去效果不好。（更复杂的模型在traning data上的表现不如简单模型的情况。）

* 在Traning data上表现不好的时候，可以考虑换一个activate function, 可以设计learning rate(adaptive learning rate)

* Sigmoid激活函数会导致gradient退化，所以在较深的网络中，Sigmoid不能用

* ReLu的好处, Fast to Compute, Biological reason, 不会退化（Leaky ReLU, Parametric ReLu）

* Maxout激活函数

* 在Traning NN的时候，error surface 可能会非常复杂，很多Adagrad可能不适用，

* RMSProp

  * $$
    w^1\leftarrow \dfrac{\eta}{\sigma^0}g^0, \sigma^0=g^0
    $$

  * $$
    w^2\leftarrow w^1 -\dfrac{\eta}{\sigma^1}g^1,\sigma^1=\sqrt{\alpha(\sigma^0)^2+(1-\alpha)(g^1)^2}
    $$

* 有人认为，在很大的NN中Local Mimima出现的概率应该是很低的。如果有出现的话可以加上momentum得到一定的解决，这就是Adam. Adam可以看做RMSProp+Momentum.
* 解决Overfitting的主要三个方法是Early Stopping, Regularization, Dropout
* Dropout:
  * 在每次updating参数之前，每个neuron都有 p%的概率被dropout
  * for each mini-batch, we resample the dropout neurons.
  * Testing: No dropout. If the dropout rate is p%, all the weights times (1-p)%.



## CNN

Why CNN?

* Some patterns are much smaller than the whole image（Convolution）

* The same patterns appear in different regions.(They can use the same set of parameters.)（Convolution）

* subsampling the pixels will not change the object.(We can subsample the pixels to make image smaller.)（Max Pooling）

  

## RNN

* Elman Network 存hidden layer的值v.s. Jordan Network 存output的值

* LSTM , forget gate关闭代表遗忘，

* RNN的error surface is rough. 一个主要问题是有些weight在不同的时间点被反复使用。

* RNN里面的activation一般还是用sigmoid.Relu的效果不好。

* LSTM可以解决gradient vanishing的问题，但是不能解决gradient explode。所以可以将learning rate调小一点。

* 当LSTM容易过拟合的时候，试试GRU.

  

