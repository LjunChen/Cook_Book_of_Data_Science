{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "无约束优化问题：常见的算法包括,'BFGS','Newton-CG','L-BFGS-B'\n",
    "\n",
    "scipy.optimize.minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proximal Gradient\n",
    "模型假设:\n",
    "$$\\min F(x)=f(x)+g(x)$$\n",
    "Assumption:\n",
    "\n",
    "1)g is proper closed and convex\n",
    "\n",
    "2)f is proper and closed, dom(f) is convex, dom(f)$\\subseteq$ int(dom(f)), and f is $L_f$-smooth over int(dom(f)). \n",
    "\n",
    "3)问题的解是非空的\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proximal Gradient Method:\n",
    "\n",
    "Initialization: pick $x^0 \\in int(dom(f))$\n",
    "\n",
    "General Step: for any $k=0,1,2,...$ excute the following steps:\n",
    "\n",
    "* a) pick $L_k>0$\n",
    "\n",
    "* b) set $x^{k+1}=prox_{\\frac{1}{L_k}g}(x^k-\\frac{1}{L_k}\\nabla f(x^k))$.\n",
    "\n",
    "这里给出prox算子的定义\n",
    "    $prox_f(x)=\\arg \\min_{u} \\{ f(u)+\\frac{1}{2}||u-x||^2\\}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用Proximal Gradient方法解决Lasso问题：\n",
    "$$\\min \\frac{1}{2} ||Y-X^T\\beta||^2+C ||\\beta||_1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(y,X,beta,C):\n",
    "    return 1/2*np.power(y-np.dot(X,beta),2).sum()+C*(np.abs(beta).sum())\n",
    "def f(y,X,beta):\n",
    "    return 1/2*np.power(y-np.dot(X,beta),2).sum()\n",
    "def subf(y,X,beta):\n",
    "    return np.dot(X.T,np.dot(X,beta)-y)\n",
    "def prox_L1(beta,C):\n",
    "    return np.sign(beta)*np.maximum(np.abs(beta)-C,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulate the data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "n=500\n",
    "p=200\n",
    "s=20\n",
    "beta=np.append(np.random.normal(0,1,s),np.zeros(p-s))\n",
    "X=np.random.normal(0,1,(n,p))\n",
    "y=np.dot(X,beta)+0.1*np.random.normal(0,1,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence reached after 26 iterations\n"
     ]
    }
   ],
   "source": [
    "def PGD_LASSO(y,X,beta,C=10,l=1,eta=1.2,maxiter=10000,conv=0.0001):\n",
    "    opt_F=[]\n",
    "    for i in range(maxiter):\n",
    "        grad_x=subf(y,X,beta)\n",
    "        while True:\n",
    "            z=prox_L1(beta-grad_x/l,C/l)\n",
    "            if  f(y,X,z)<f(y,X,beta)+np.dot(grad_x,z-beta)+l/2*np.power(z-beta,2).sum():\n",
    "                break\n",
    "            l=eta*l\n",
    "        beta=z\n",
    "        opt_F.append(F(y,X,beta,C))\n",
    "        if i>1 and np.abs(opt_F[i]-opt_F[i-1])<conv:\n",
    "            print('convergence reached after {} iterations'.format(i))\n",
    "            break\n",
    "        if i==(maxiter-1):\n",
    "            print('algorithm fails to converge')\n",
    "    return beta,opt_F\n",
    "beta,opt_F=PGD_LASSO(y,X,beta=np.random.normal(0,1,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "带约束的优化问题\n",
    "\n",
    "1)QP问题可以直接用拉格朗日法求解\n",
    "\n",
    "2）ADMM算法\n",
    "$$H_{opt}=\\min \\{ H(x,z)=h_1(x)+h_2(z): Ax+Bz=c\\}$$.\n",
    "\n",
    "这里我们假设: $h_1$和$h_2$ are proper closed and convex functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADMM\n",
    "\n",
    "Initialization : $x^0\\in R^n, z^0 \\in R^p, y^0\\in R^m ,\\rho>0$\n",
    "\n",
    "General Step: for any $k=0,1,...$ excute the following:\n",
    "\n",
    "(a) $x^{k+1}\\in \\arg\\min_x \\{ h_1(x)+\\frac{\\rho}{2} ||Ax+Bz^k-c+\\frac{1}{\\rho}y^k||^2\\}$\n",
    "\n",
    "(b)$z^{k+1}\\in \\arg\\min_z \\{ h_2(z)+\\frac{\\rho}{2} ||Ax^{k+1}+Bz-c+\\frac{1}{\\rho}y^k||^2\\}$\n",
    "\n",
    "(c) $y^{k+1}=y^k+\\rho(Ax^{k+1}+Bz^{k+1}-c)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AD-LPMM\n",
    "\n",
    "Initialization: $x^0\\in R^n, z^0 \\in R^p, y^0\\in R^m ,\\rho>0, \\alpha \\ge \\rho \\lambda_{max}(A^TA),\\beta\\ge \\rho\\lambda_{max}(B^TB)$\n",
    "\n",
    "General Step: for any $k=0,1,...$ excute the following:\n",
    "\n",
    "(a) $x^{k+1}=prox_{\\frac{1}{\\alpha}h_1} [x^k-\\frac{\\rho}{\\alpha}A^T(Ax^k+Bz^k-c+\\frac{1}{\\rho}y^k)]$\n",
    "\n",
    "(b) $z^{k+1}=prox_{\\frac{1}{\\beta}h_2}[z^k-\\frac{\\rho}{\\alpha}B^T(Ax^{k+1}+Bz^k-c+\\frac{1}{\\rho}y^k)]$\n",
    "\n",
    "(c) $y^{k+1}=y^k+\\rho(Ax^{k+1}+Bz^{k+1}-c)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
